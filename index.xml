<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Asa</title>
    <link>http://asasorano.fun/</link>
    <description>Recent content on Asa</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>Asa</copyright>
    <lastBuildDate>Wed, 17 Jan 2024 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="http://asasorano.fun/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>About</title>
      <link>http://asasorano.fun/about/</link>
      <pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>http://asasorano.fun/about/</guid>
      
        <description>&lt;blockquote&gt;
&lt;p&gt;The black hole takes all and gives back naught. The anguish from a sudden, untimely death has a narrowing effect: alternatives are lost, space in my mind, too.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Literally, This journal serves as a back-up for my life, offering a personal and serene space for reflection and self-expression.&lt;/p&gt;
&lt;p&gt;I want to be a warm haven for those who cross paths with me. When they in moments of anxiety or sorrow, they maybe feel at ease to recall our shared memories and engage in open conversations with me.&lt;/p&gt;
&lt;p&gt;Honestly, that like a Taxi-driver.&lt;/p&gt;
&lt;p&gt;Indeed, I feel gratitude to do that because it bolsters my self-esteem and sustains me through a life of quietude and solitude, illuminated by flickers of hope and anticipation for the future. To prove that I possess unique strengths and inner spark, a reminder that I, too, can be cherished or memorized.&lt;/p&gt;
&lt;p&gt;That’s enough.&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;不合时宜的死亡如同黑洞的坍缩, 与之相关的人无一幸免, 那一瞬的痛苦带来的伤害不断发酵, 不断重复. 随着麻木的你一同到来的是对于生活可能性的追求, 所有的本该发生的幸福在梦里不断上演, 直至内心归于沉寂.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我将这个博客视作自己的二重人生, 一份当时所感所想的备份, 时间机器. 在这私密的地方, 我可以说所有我想说的, 不被任何东西所约束.&lt;/p&gt;
&lt;p&gt;我将自己在亲密关系里的定位为一个收听者, 当朋友们在因悲伤回忆起我时, 我希望我是足以让他们不带有疲惫感的与我交流的人.&lt;/p&gt;
&lt;p&gt;事实上, 这很像一个计程车司机.&lt;/p&gt;
&lt;p&gt;我将这个身份或者能力视作一种追求, 因为我的自信也因此而建立, 支撑我走过每一个孤独与安静的日与夜, 照亮我的生活, 让我对未来充满希冀. 因为这证明我是值得被很好的回忆的人.&lt;/p&gt;
&lt;p&gt;那便足够了.&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;&lt;img src=&#34;http://tva1.sinaimg.cn/large/006gEUdsly1hlfp2gd6qjj30qo0qon4h.jpg#pic_center&#34; alt=&#34;selffie.jpg&#34;&gt;
Asa (QIAO Ke)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Based in Beijing&lt;/li&gt;
&lt;li&gt;Specialized in Information Management and Information System&lt;/li&gt;
&lt;li&gt;Interested in AI, Data Science, HCI, Web development, Letters, Movies, Sports&lt;/li&gt;
&lt;li&gt;Companies with Jack Russell Terrier named “Tang Yuan(sweet dumpling)”&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;自我介绍&#34;&gt;自我介绍&lt;/h1&gt;
&lt;p&gt;乔轲&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;住在北京&lt;/li&gt;
&lt;li&gt;信息管理与信息系统专业&lt;/li&gt;
&lt;li&gt;对于人工智能, 数据科学, 人机交互, 网页开发, 文学, 电影, 运动都很感兴趣&lt;/li&gt;
&lt;li&gt;和父母一起养着一只杰克罗素梗 名叫“汤圆”&lt;/li&gt;
&lt;/ul&gt;
</description>
      
    </item>
    
    <item>
      <title>Introduction</title>
      <link>http://asasorano.fun/post/cs229n/</link>
      <pubDate>Wed, 17 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>http://asasorano.fun/post/cs229n/</guid>
      
        <description>&lt;h1 id=&#34;intro&#34;&gt;Intro&lt;/h1&gt;
&lt;p&gt;Meaning: signifier(symbol)↔signified(idea or thing) =denotational semantics&lt;/p&gt;
&lt;h3 id=&#34;wordnet&#34;&gt;WordNet&lt;/h3&gt;
&lt;p&gt;A thesaurus containing lists of synonym sets and hypernyms(”is a” relationships)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; nltk.corpus &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; wordnet &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; wn
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;poses &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; { &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;n&amp;#39;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;noun&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;v&amp;#39;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;verb&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;s&amp;#39;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;adj (s)&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;a&amp;#39;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;adj&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;r&amp;#39;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;adv&amp;#39;&lt;/span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; synset &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; wn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;synsets(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;good&amp;#34;&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(poses[synset&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pos()], 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;, &amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join([l&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;name() &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; l &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; synset&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lemmas()])))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;noun: good
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;noun: good, goodness
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;noun: good, goodness
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;noun: commodity, trade_good, good
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;adj: good
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;adj (s): full, good
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;adj: good
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;adj (s): estimable, good, honorable, respectable
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;adj (s): beneficial, good
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;adj (s): good
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;adj (s): good, just, upright
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;adj (s): adept, expert, good, practiced, proficient, skillful, skilful
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;adj (s): good
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;adj (s): dear, good, near
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;adj (s): dependable, good, safe, secure
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;adj (s): good, right, ripe
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;adj (s): good, well
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;adj (s): effective, good, in_effect, in_force
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;adj (s): good
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;adj (s): good, serious
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;adj (s): good, sound
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;adj (s): good, salutary
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;adj (s): good, honest
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;adj (s): good, undecomposed, unspoiled, unspoilt
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;adj (s): good
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;adv: well, good
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;adv: thoroughly, soundly, good
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; nltk.corpus &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; wordnet &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; wn
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;panda &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; wn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;synset(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;panda.n.01&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hyper &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; s: s&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;hypernyms()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;list(panda&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;closure(hyper))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[Synset(&amp;#39;procyonid.n.01&amp;#39;),
 Synset(&amp;#39;carnivore.n.01&amp;#39;),
 Synset(&amp;#39;placental.n.01&amp;#39;),
 Synset(&amp;#39;mammal.n.01&amp;#39;),
 Synset(&amp;#39;vertebrate.n.01&amp;#39;),
 Synset(&amp;#39;chordate.n.01&amp;#39;),
 Synset(&amp;#39;animal.n.01&amp;#39;),
 Synset(&amp;#39;organism.n.01&amp;#39;),
 Synset(&amp;#39;living_thing.n.01&amp;#39;),
 Synset(&amp;#39;whole.n.02&amp;#39;),
 Synset(&amp;#39;object.n.01&amp;#39;),
 Synset(&amp;#39;physical_entity.n.01&amp;#39;),
 Synset(&amp;#39;entity.n.01&amp;#39;)]
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;problems-of-wordnet&#34;&gt;Problems of WordNet&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;missing nuance e.g. “proficient” and “good” is a synonym, this is only correct in some contexts.&lt;/li&gt;
&lt;li&gt;missing new meanings of words&lt;/li&gt;
&lt;li&gt;subjective&lt;/li&gt;
&lt;li&gt;require human labor to create and adapt&lt;/li&gt;
&lt;li&gt;can’t computer accurate word similarity&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;representing-words-as-discrete-symbols&#34;&gt;Representing words as discrete symbols&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;one-hot&lt;/li&gt;
&lt;li&gt;these two vectors are orthogonal&lt;/li&gt;
&lt;li&gt;no-similarity&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;representing-words-by-their-context&#34;&gt;Representing words by their context&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Distributional semantics: &lt;strong&gt;A word’s meaning is given by the words that frequently appear close-by&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;word-vectorsword-embeddings&#34;&gt;Word vectors(word embeddings)&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://prod-files-secure.s3.us-west-2.amazonaws.com/edb26675-ea0b-439d-b7bd-2214cc9e7c5d/be5b2ad0-c362-40a9-a3c3-53ad2751ac61/%E6%88%AA%E5%B1%8F2024-01-14_10.10.16.png&#34; alt=&#34;截屏2024-01-14 10.10.16.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Idea:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We have a large corpus of text&lt;/li&gt;
&lt;li&gt;Every word in a fixed vocabulary is represented by a vector&lt;/li&gt;
&lt;li&gt;Go through each position &lt;em&gt;t&lt;/em&gt; in the text, which has a center word &lt;em&gt;c&lt;/em&gt; and context (“outside”) words &lt;em&gt;o&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Use the similarity of the word vectors for &lt;em&gt;c&lt;/em&gt; and &lt;em&gt;o&lt;/em&gt; to calculate the probability of &lt;em&gt;o&lt;/em&gt; given &lt;em&gt;c&lt;/em&gt; (or vice versa)&lt;/li&gt;
&lt;li&gt;Keep adjusting the word vectors to maximize this probability&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://prod-files-secure.s3.us-west-2.amazonaws.com/edb26675-ea0b-439d-b7bd-2214cc9e7c5d/ca4e1794-636a-4330-a706-3a1411ba8870/%E6%88%AA%E5%B1%8F2024-01-14_10.15.00.png&#34; alt=&#34;截屏2024-01-14 10.15.00.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;objective-function&#34;&gt;Objective function&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Minimizing objective function ⟺ Maximizing predictive accuracy&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
J(\theta) = -\frac{1}{T}\log L(\theta) = -\frac{1}{T} \sum_{t=1}^{T} \sum_{\substack{-m \leq j \leq m \ j \neq 0}} \log P(w_{t+j} | w_t; \theta)
$$&lt;/p&gt;
&lt;p&gt;So how to calculate the P?&lt;/p&gt;
&lt;p&gt;We will &lt;em&gt;use two&lt;/em&gt; vectors per word &lt;em&gt;w&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;v_w when &lt;em&gt;w&lt;/em&gt; is a center word&lt;/li&gt;
&lt;li&gt;u_w when &lt;em&gt;w&lt;/em&gt; is a context word&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then for a center word &lt;em&gt;c&lt;/em&gt; and a context word &lt;em&gt;o&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;$$
P(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in V} \exp(u_w^T v_c)}
$$&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://prod-files-secure.s3.us-west-2.amazonaws.com/edb26675-ea0b-439d-b7bd-2214cc9e7c5d/2fad4fea-dd3e-4af4-aa27-e935a0fdd512/%E6%88%AA%E5%B1%8F2024-01-14_10.36.26.png&#34; alt=&#34;截屏2024-01-14 10.36.26.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why similarity could be seen as possibility?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Similarity combines both the magnitude and the orientation of the vectors.&lt;/li&gt;
&lt;li&gt;A high dot product value implies a high degree of similarity or compatibility, suggesting that the context word is likely to appear near the target word.&lt;/li&gt;
&lt;li&gt;Then softmax function is used to turn raw similarity scores (like the dot products) into probabilities.&lt;/li&gt;
&lt;li&gt;Similarity between word vectors can be thought of as a proxy for the probability of co-occurrence.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://prod-files-secure.s3.us-west-2.amazonaws.com/edb26675-ea0b-439d-b7bd-2214cc9e7c5d/17428153-c0c2-4f56-82b8-c0dec2731273/%E6%88%AA%E5%B1%8F2024-01-14_10.44.34.png&#34; alt=&#34;截屏2024-01-14 10.44.34.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Actually, this is an example of the &lt;strong&gt;softmax function ℝ&lt;/strong&gt; &lt;strong&gt;→ (0,1)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
\text{softmax}(x_i) = \frac{\exp(x_i)}{\sum_{j=1}^{n} \exp(x_j)} = p_i
$$&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;softmax function&lt;/strong&gt; maps arbitrary values x_i to a probability distribution p_i&lt;/p&gt;
&lt;p&gt;You may wanna ask: why dot product can compare the similarity of o and c?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Projection&lt;/strong&gt;: The dot product of two vectors is the magnitude of the projection of one vector onto another.&lt;/p&gt;
&lt;p&gt;e.g. vector k is king; vector a is apple; vector q is queen&lt;/p&gt;
&lt;p&gt;$\vec{k}\cdot \vec{a}=[2,3]⋅[5,−1]=(2×5)+(3×−1)=10−3=7$&lt;/p&gt;
&lt;p&gt;$\vec{k}\cdot \vec{q}==[2,3]⋅[2,2.5]=(2×2)+(3×2.5)=4+7.5=11.5$&lt;/p&gt;
&lt;h3 id=&#34;training-a-model-by-optimizing-parameters&#34;&gt;Training a model by optimizing parameters&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://prod-files-secure.s3.us-west-2.amazonaws.com/edb26675-ea0b-439d-b7bd-2214cc9e7c5d/e15319e1-c30e-4205-a60a-d1fe9d54c6d5/Untitled.png&#34; alt=&#34;Untitled&#34;&gt;&lt;/p&gt;
&lt;p&gt;$\theta$  represents all model parameters, in one long vector&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://prod-files-secure.s3.us-west-2.amazonaws.com/edb26675-ea0b-439d-b7bd-2214cc9e7c5d/bb42d9a1-b5f2-4862-bf69-bfbd4a0adf95/Untitled.png&#34; alt=&#34;Untitled&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Remember: every word has two vectors&lt;/li&gt;
&lt;li&gt;We optimize these parameters by walking down the gradient&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Take an example of calculating walking down the gradient of center words c. (also need to consider a context word o)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://prod-files-secure.s3.us-west-2.amazonaws.com/edb26675-ea0b-439d-b7bd-2214cc9e7c5d/1adb9dda-0195-4033-b270-8ab1a45a008a/%E6%88%AA%E5%B1%8F2024-01-14_15.43.09.png&#34; alt=&#34;截屏2024-01-14 15.43.09.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;here is a useful basic fact:$\frac{\partial \mathbf{x}^T a}{\partial \mathbf{x}} = \frac{\partial a^T \mathbf{x}}{\partial \mathbf{x}} = a$&lt;/p&gt;
&lt;p&gt;chain rule! If &lt;em&gt;y&lt;/em&gt; = &lt;em&gt;f&lt;/em&gt;(&lt;em&gt;u&lt;/em&gt;) and &lt;em&gt;u&lt;/em&gt; = &lt;em&gt;g&lt;/em&gt;(&lt;em&gt;x&lt;/em&gt;), i.e., &lt;em&gt;y&lt;/em&gt; = &lt;em&gt;f&lt;/em&gt;(&lt;em&gt;g&lt;/em&gt;(x)), then:&lt;/p&gt;
&lt;p&gt;$\frac{dy}{dx} = \frac{dy}{du} \frac{du}{dx} = \frac{df(u)}{du} \frac{dg(x)}{dx}$&lt;/p&gt;
&lt;p&gt;Two model variants:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Skip-grams (SG)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Predict context (“outside”) words (position independent) given center word&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Continuous Bag of Words (CBOW)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Predict center word from (bag of) context words&lt;/p&gt;
&lt;p&gt;This lecture so far: &lt;strong&gt;Skip-gram model&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Additional efficiency in training:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Negative sampling&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So far: Focus on &lt;strong&gt;naïve softmax&lt;/strong&gt; (simpler but more expensive training method)&lt;/p&gt;
&lt;h3 id=&#34;optimization-gradient-descent&#34;&gt;Optimization: Gradient Descent&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We have a cost function $J(\theta)$we want to minimize&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gradient Descent&lt;/strong&gt; is an algorithm to minimize $J(\theta)$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Idea:&lt;/strong&gt; for current value of $\theta$, calculate gradient of $J(\theta)$ , then take small step in direction of negative gradient. Repeat.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://prod-files-secure.s3.us-west-2.amazonaws.com/edb26675-ea0b-439d-b7bd-2214cc9e7c5d/2e87b2cf-af9a-4e04-9035-6f1ce242c644/Untitled.png&#34; alt=&#34;Untitled&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;negative-sampling&#34;&gt;Negative sampling&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Technique used to reduce the computational complexity&lt;/li&gt;
&lt;li&gt;In a typical language model, you predict the probability of a word given its context, which requires normalizing the output probabilities across the entire vocabulary using a softmax function. This normalization step can be very computationally expensive because it involves summing over all words in the vocabulary for each training example.&lt;/li&gt;
&lt;li&gt;Negative sampling addresses this issue by simplifying the problem to a binary classification task for each context word. Instead of predicting the probability of the target word amongst the entire vocabulary (multiclass classification), the model predicts the probability that a word and a context are seen together (positive examples) versus the probability that they are not (negative examples).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;e.g.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The quick brown fox jumps over the lazy dog.”&lt;/p&gt;
&lt;p&gt;we choose &amp;ldquo;brown&amp;rdquo; as the target word, and we&amp;rsquo;re using a context window size of 1, then our context words are &amp;ldquo;quick&amp;rdquo; and &amp;ldquo;fox&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Positive Samples:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In our training data, we would have positive samples that are actual context-target pairs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(quick, brown)&lt;/li&gt;
&lt;li&gt;(brown, fox)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Negative Sampling:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;we choose to generate 2 negative samples for each positive pair. Here&amp;rsquo;s how we might generate negative samples:&lt;/p&gt;
&lt;p&gt;For the positive pair (quick, brown), negative samples could be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(quick, dog)&lt;/li&gt;
&lt;li&gt;(quick, lazy)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the positive pair (brown, fox), negative samples could be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(fox, the)&lt;/li&gt;
&lt;li&gt;(fox, jumps)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Training the Model:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;During training, the Word2Vec model will adjust its internal word representations to do two things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Increase the probability that the true context words (&amp;ldquo;quick&amp;rdquo; and &amp;ldquo;fox&amp;rdquo;) are predicted given the target word &amp;ldquo;brown&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;Decrease the probability that the randomly selected negative samples (&amp;ldquo;dog&amp;rdquo;, &amp;ldquo;lazy&amp;rdquo;, &amp;ldquo;the&amp;rdquo;, &amp;ldquo;jumps&amp;rdquo;) are predicted given the context word.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is typically done by using a logistic regression model that outputs a probability between 0 and 1. For positive samples, the model aims to output a probability close to 1, and for negative samples, it aims to output a probability close to 0.&lt;/p&gt;
&lt;p&gt;By repeating this process across all the words in a large corpus, the model effectively learns word embeddings that capture semantic similarities and relationships between words, because words that appear in similar contexts will end up having similar word embeddings. Negative sampling makes this training process much more efficient, especially when dealing with large vocabularies.&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>We need romantic expression in our daily lives.</title>
      <link>http://asasorano.fun/post/blog1/</link>
      <pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>http://asasorano.fun/post/blog1/</guid>
      
        <description>&lt;p&gt;Standing behind the closed window door of the subway, I gaze at the swiftly passing train and the fleeting shadow of people, I wonder: why I am a human? With an overwhelming disgusting surge in my stomach, I feel my soul imprisoned in the ugly physical form. I should and I could be more free. I find myself and comfort that: latter, latter, be patient, please.&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>You were never afraid of the waters or the many eyes on the cypress tree trunks.</title>
      <link>http://asasorano.fun/post/blog0/</link>
      <pubDate>Wed, 27 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>http://asasorano.fun/post/blog0/</guid>
      
        <description>&lt;p&gt;Dear&lt;/p&gt;
&lt;p&gt;绝大部分的人都很有趣, 我喜欢去幻想他们的生活, 困顿, 和信念。就好像是自己在写小说一样, 而在生活中小说的人物鲜活的在我面前走过, 我会好奇那个立体的他们, 敏感, 孤立的他们. 但是这一切的只会也只能发生在我的大脑中, 我不擅长和陌生人沟通, 最基本的礼貌交流可以做的到, 然而如果更进一步, 也许仅仅只是一步, 我就有将自己的想法全盘托出的冲动. 我知道这是一种“危险”的事情, 是因为在别人看来甚至会歇斯底里, 从我的经验来看, 无论多么亲密的关系(也许相知甚少反而会截然不同), 在触及一些敏感的话题时, 是更倾向去逃避的, 可能因为大家都不喜欢价值观的碰撞, 甚至共鸣也有所不信任, 而这其中的原因, 也许只能质问未来的自己.&lt;/p&gt;
&lt;p&gt;越是临近毕业, 越是忙碌. 甚至会发出疑问: “为什么要将最重要最忙碌的事情安排在临近分别的尾声呢? 真是不理解规则制定者的想法”, 仔细一想, 他大概也理解不了我. 就是这样过分的忙碌 麻痹了我们的感官, 隔一段时间会感觉自己好像完全抽出的绳索, 需要一点时间将它重新收束回来. 因此为了降低消耗, 降低需要去收拾自己的周期, 带着“边界感”我去参加每一个需要自己去面对的事情. 想象着自己是少年卡夫卡, 认真坚毅的经营自己的生活.&lt;/p&gt;
&lt;p&gt;说起边界感, 上学期艺术疗愈老师为了帮做毕设的学生收集素材, 让我们用自己的画笔描绘边界感. 以往很难下笔的我几乎是瞬间得到的答案, 我很少去相信这瞬间的结果, 通常要去再次揣摩是否有不妥. 但是那次, 我很清晰的意识到, 那就是我的答案, 它在我身体里已经生根发芽已久, 所以才会本能的反应. 我的答案是: 边界感就是一边说“你好”, 一边说“再见”. 在足够长的心理预期的时间内, 或者预期之内这个世界不会发生天翻地覆的改变的情况下, 假设再见面时, 彼此仍是彼此, 正是有着这些假设所以不会珍惜每一次的遇见, 每一秒彼此河流交汇的瞬间. 这对自己的生活会不会算是一种不负责? 走在路上, 厚厚的冬装包裹, 觉得自己像是一只狗熊.&lt;/p&gt;
&lt;p&gt;每每短暂的相识在将别之际, 我都会告诉自己—“这是最后一次见面了.” 但这只不过是内心的窃窃私语, 最后一次见面也好, 第一次见面也罢, 河流只会不断的向前走, 我能做的只有记录, 在不足够清醒时回忆, 甚至不敢做到思念, 身体力行的去追忆什么.&lt;/p&gt;
&lt;p&gt;“我们要! 一起走!”, 当然会有这种冲动, 去努力的维系, 自大的认为时间地点这些在如今的时代很容易克服, 但实际去做会发现, 如果我们的生活, 我们的时间没有足够的重合, 那便谈不上维系. 即在交际的影响因素中时间与空间的重合度比心灵的契合度更为重要. 再去面对这段关系时那种奇怪的疏离感会将礼貌的你我带入, 又回到舒适的壳. 归根到底是 我们真正能重视的东西太少了.&lt;/p&gt;
&lt;p&gt;如果再次回到那时的坦然, 我们可以做的只有再次回到那个时间, 短暂的扮演下过去的自己. 这也是在和初中同学的一次聚会中学到的. 我几乎没有熬过通宵, 但那次住在山里, 我才知道日出时, 在未见太阳之前, 太阳散射的光就足以将世界的轮廓勾勒出. 山里起了雾气, 四周朦胧的灰蓝, 坐在阳台, 微弱的光让我勉强看清这个世界, 但一切都在沉睡, 在如同静止的景象中, 每一次呼吸都意识到时间的前进, 不知是不是酒精在我体内仍在发生作用, 我有一丝的晕厥与恶心. 等天亮了, 就又要回归自我了, 我这样想着. 跳入过去就如同逃入别人的人生一般.&lt;/p&gt;
&lt;p&gt;最后一节在本科的课程, 幸运的是一节英语选修“学术英语口语”, 安排是5分钟的学术演讲分享自己的小研究, 比起考试或者散漫的听一节课, 这种还算的上有仪式感. 更何况在课程中认识到了一批比我小一届的同学, 大家一起合作的很愉快. 特别有一位在课题上竟有些相似, 之前还在课后专门交谈了很久. 总的来说是可以充满期待与能量去面对的课程. 只不过是最后一次课的特殊标签, 我耍赖式的, 见到她就提出: “嘿! 这是大学的最后一节课啦!”, 不好说出口的话如果以开玩笑的心态去说出来, 就会简单很多, 这个技巧支撑了我人生中的很多重要时刻. 课上我们在教室的最后一排小声的交流了很久, 完全没有管他们台上在说什么, 却鼓了全场最响亮的掌声.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“nothing will gonna be the same again, right?”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;临近岔路口将要分别时, 充满技巧性的我说: “最后一次见面啦!”. 有点忘了最后的一段交谈逻辑了, 只记得说下次一起带我打麻将, 和我说我根本不会打麻将. 还有我已经转头准备大步往自己的方向走时, 听到后面叫我说, 因为这是最后一面, 再看你一眼. 我回了一句 哈?不还打麻将呢吗?&lt;/p&gt;
&lt;p&gt;放心吧, 我也会记得米饭加一个无菌蛋加上一点麻酱会很好吃的啦.&lt;/p&gt;
&lt;p&gt;今年总体上是最少去回忆的一年, 因为我下定决心要“逃走”, 坚定了自己目前的命运就是远行, 所以不断想象着下一步的生活, 每天的任务接踵而至. 但在前几天不断下雪的冬夜回宿舍的路上还是会刻意的绕远, 踩在厚厚的仍未被人踏足的雪上, 飘落的雪已经在羽绒帽子上积了一层, 在没有人的湖边一边听歌一边游离于精神之间, 让身体于精神驻足一下, 让坚硬的内心松软一点. 那时我能想起的, 都是我生命中的养分, 生命的奶油, 不断的分裂, 碰撞, 给予我热量, 支撑我继续走下去.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“如果有天汇入大海,你会记得我吗?”, “如果没有明天, 请你现在亲吻我吧.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;想起了年中在医院里, 我能有百分百自信绝对没有事情, 翘首以盼回到正常的生活原因. 一方面是食欲真的特别特别好, 这首先从物质层面就是不会出问题的因素以外, 就是当时在写艺术疗愈的结课作业时不断在回忆给予我力量的艺术和人, 和每一位看望我的朋友, 送来的花束, 这些赠礼完全足以支撑我好好的, 因为未来太过于美好, 有他们所在的这个世界太令我痴迷, 而这种渴望是仅凭自己无法生成的.&lt;/p&gt;
&lt;p&gt;所以2023的最后几天, 明天开完组会, 迫在眉睫的事情就暂告一段落. 想去逛逛日用品店, 给朋友包几个简单的礼物, 和约定很久的朋友见一面, 然后就打算回到家, 和父母, 爷爷, 和狗狗一起, 今年最后一次的归来自己的内心.&lt;/p&gt;
&lt;p&gt;可以说以往的所有时光里, 我都在不断注视着自己的内心, 但明年的愿景是向外发散, 研究外在的世界, 既然如此喜欢, 那就更加了解它一点. 相信这个不断拓展外在的过程也会不断让我更加看清我的内心, 或者说如今的阶段, 只有这样做才能进一步的看清自己的内心. 搭建起内在理性, 安心这不会与敏感脆弱的的你相矛盾, 或者说矛盾的才是对的.&lt;/p&gt;
&lt;p&gt;愿一切顺利, 离梦想更进一步.&lt;/p&gt;
&lt;p&gt;Sincerely,&lt;/p&gt;
&lt;p&gt;Asa&lt;/p&gt;
&lt;p&gt;2023.11.27&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>Taxi-driver</title>
      <link>http://asasorano.fun/post/blog2/</link>
      <pubDate>Fri, 17 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>http://asasorano.fun/post/blog2/</guid>
      
        <description>&lt;p&gt;我也想象过打开那扇门, 我以一个平等的角度面对并爱着每一个人, 没有仇恨, 嫉妒, 猜疑,  我可以没有理由的不在意一切, 向所有人随机的倾诉一切, 多么可爱又不存在威胁, 是团体里的弱连接, 不可或缺又不拥有决定性.&lt;/p&gt;
&lt;p&gt;但很显然我打不开, 我生来不是那样的人意味着我此生会这样的人吸引(不是仅仅选择传统简单又武断的人), 即使我知道他们也会面对一些什么产生苦恼和迟疑, 丢失以往的游刃有余, 但我会不由分说的将其看作可爱的缺憾, 那一份瑕疵的美就像是拥有故事感一般的破败饱经风霜的美一样让我向往, 想成为, 想拥有.&lt;/p&gt;
&lt;p&gt;所以我想说, 那我成为高手? 不需要任何链接, 我个人的自洽容不得别人来掺杂的一点杂质, 你的恶意来自于你自身的庸人自扰, 我像是修行的石头一般不为所动, 而且越来越好. 但我的智商不支持我成为一个高手, 我的敏感不支持我成为一块石头. 而不同的却相似的刺痛我自尊心的纷扰, 却实实在在的让我形成了保护性机制. 我已经训练好受伤后受伤者的不顾一切的眼神, 我也已经训练好受伤后强撑起内心的思路. 这当然是拙略的演技, 我相信明眼人一眼都能看穿, 我都不得不怀疑是不是这一切是不知道的我留下的破绽, 漏洞, 因为知道自己无法凭空制造出神秘感, 进而等待着别人来关心关心我, 来尝试调动别人的同情心, 来简单的爱我一下.&lt;/p&gt;
&lt;p&gt;很显然我都没有做好, 我在自身于外界之间建立不起一个正向的的交互, 所以我不断的说对不起, 不断的写我自己说对不起, 好像我在不断的表达我是弱者, 我有问题, 我很脆弱, 所以不要来碰我, 不要欺负我, 如果可以请善待我, 用你的真心.&lt;/p&gt;
&lt;p&gt;清醒之时, 亦或是直面自己之时&lt;/p&gt;
&lt;p&gt;傻子, 笨蛋, 庸俗至极.&lt;/p&gt;
&lt;p&gt;那么如果我先天性的缺失那份自由的活力, 我想最理想的是选择我可以作为任何故事中开车的那个人, 开的是计程车, 我不掌握方向, 我想引出话题, 听我想听的, 记录与我接触的每个人, 我这样我既是高手也是一个弱连接的角色. 我舒适的在自己的身边构筑起高墙, 挖一个洞, 窥视着所有人的人生, 当然我不会容忍自己对任何乘客发出负面评价, 也不会容忍自己为了可以与心仪的乘客多待一会而降低我行驶的速度.&lt;/p&gt;
</description>
      
    </item>
    
  </channel>
</rss>
